{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "m_vHBVHigp3S",
    "outputId": "730a6711-febe-4e0e-fab3-4f41f429a7f0"
   },
   "outputs": [],
   "source": [
    "#Optional code to download Glove pretrained Embeddings\n",
    "# from urllib.request import urlretrieve\n",
    "# import os\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# def download(url, file):\n",
    "#     if not os.path.isfile(file):\n",
    "#         print(\"Download file... \" + file + \" ...\")\n",
    "#         urlretrieve(url,file)\n",
    "#         print(\"File downloaded\")\n",
    "\n",
    "# download('http://nlp.stanford.edu/data/glove.6B.zip','Glove.zip')\n",
    "# print(\"All the files are downloaded\")\n",
    "# def uncompress_features_labels(dir):\n",
    "#     if(os.path.isdir('data')):\n",
    "#         print('Data extracted')\n",
    "#     else:\n",
    "#         with ZipFile(dir) as zipf:\n",
    "#             zipf.extractall('data')\n",
    "# uncompress_features_labels('Glove.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "_4Tx5vM525pl",
    "outputId": "e8c8f370-477d-40e6-a388-e524e9cb2e49"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Embedding,Dense,LSTM,CuDNNLSTM,Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "from random import randint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ltv3Ysz2KGr"
   },
   "outputs": [],
   "source": [
    "#Function to load corpus\n",
    "def load_text(filename):\n",
    "  file=open(filename,'r')\n",
    "  text=file.read()\n",
    "  file.close()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxgnBD5r-ioR"
   },
   "source": [
    "# Word Based Text Generation for Autocompleting Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAodIo0A_6Id"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "MAX_LEN = 10\n",
    "MIN_WORDS = 4\n",
    "\n",
    "PADDING = 'post'\n",
    "TRUNCATING = 'pre'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZNypwoTxA1R"
   },
   "outputs": [],
   "source": [
    "#Function for preprocessing the text\n",
    "def clean_text_word(text):\n",
    "  text=text.replace('.',' .')\n",
    "  tokens = text.split()\n",
    "  punc=string.punctuation\n",
    "  punc=punc.replace('.','')\n",
    "  punc=punc + str(\"\\\\\")\n",
    "  table=str.maketrans('','',punc)\n",
    "  tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "  tokens = (word for word in tokens if not word.isdigit())\n",
    "\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMbR93DVytEj"
   },
   "outputs": [],
   "source": [
    "#Function for separating all sentences from the text\n",
    "def generate_sentences(tokens):\n",
    "  sentences=list()\n",
    "  sentence=list()\n",
    "  for word in tokens:\n",
    "    if(word=='.'):\n",
    "      sentence.append(word)\n",
    "      sentences.append(sentence)\n",
    "      sentence=list()\n",
    "    else:\n",
    "      sentence.append(word)\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf-bkkn4CoK2"
   },
   "outputs": [],
   "source": [
    "#Function for generating sequences which is used to build the train data\n",
    "def generate_sequences(sentences):\n",
    "  sequences=list()\n",
    "  split=1/3\n",
    "  for sentence in sentences:\n",
    "    for i in range(int(len(sentence)*split),len(sentence)):\n",
    "    # print(i)\n",
    "    # print(i-int(len(sentence)/4))\n",
    "      seq=sentence[i-int(len(sentence)*split):i+1]\n",
    "      line=' '.join(seq)\n",
    "      sequences.append(line)\n",
    "  return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nf0W4cD1cGGK"
   },
   "outputs": [],
   "source": [
    "raw_text=load_text('internet_archive_scifi_v3.txt')\n",
    "tokens=clean_text_word(raw_text)\n",
    "\n",
    "sentences=generate_sentences(tokens[700:2000000])\n",
    "\n",
    "sequences=generate_sequences(sentences)\n",
    "\n",
    "tokenizer=Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~\\t\\n',num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "sen_sequences=tokenizer.texts_to_sequences(sequences)\n",
    "sen_sequences=np.array(sen_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mheF_WMephR"
   },
   "outputs": [],
   "source": [
    "X,y=list(),list()\n",
    "for sentence in sen_sequences:\n",
    "  if(len(sentence)<MIN_WORDS): #Asserting minimum number of words in a sentence\n",
    "    continue\n",
    "  X.append(sentence[:-1])\n",
    "  y.append(sentence[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSrPRZWueurf"
   },
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "y=y.reshape((-1,1))\n",
    "X_pad=list()\n",
    "#Padding all train data to have equal length\n",
    "for data in X:\n",
    "  pad=pad_sequences([data],maxlen=MAX_LEN,padding=PADDING,truncating=TRUNCATING)[0]\n",
    "  X_pad.append(pad)\n",
    "\n",
    "X_pad=np.array(X_pad)\n",
    "seq_length=X_pad.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZ9KTiiSrEBI"
   },
   "outputs": [],
   "source": [
    "#Function for loading the pretrained glove embeddings\n",
    "# def load_glove_embeddings(text_location):\n",
    "#   glove_embeddings={}\n",
    "#   f=open(text_location)\n",
    "#   for line in f:\n",
    "#     values=line.split()\n",
    "#     word=values[0]\n",
    "#     coefs=np.asarray(values[1:],dtype='float32')\n",
    "#     glove_embeddings[word]=coefs\n",
    "#   f.close()\n",
    "#   return glove_embeddings\n",
    "\n",
    "# glove_embeddings=load_glove_embeddings('data/glove.6B.200d.txt')\n",
    "\n",
    "# EMBEDDING_DIM=200 #Change according to the emedding dimension of the file used\n",
    "# word_index=tokenizer.word_index\n",
    "# glove_matrix=np.zeros((len(word_index)+1,EMBEDDING_DIM))\n",
    "# for word,i in word_index.items():\n",
    "#   vector=glove_embeddings.get(word)\n",
    "#   if vector is not None:\n",
    "#     glove_matrix[i]=vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNpPeMMFkd_r"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "# model.add(Embedding(len(word_index)+1,EMBEDDING_DIM,weights=[glove_matrix],input_length=seq_length,trainable=False))\n",
    "model.add(Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=seq_length))\n",
    "model.add(CuDNNLSTM(100,return_sequences=True))\n",
    "model.add(CuDNNLSTM(100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(100,return_sequences=True))\n",
    "model.add(CuDNNLSTM(100,return_sequences=True))\n",
    "model.add(CuDNNLSTM(100))\n",
    "#Use the commented code instead of CuDNN if using CPU to train \n",
    "# model.add(LSTM(128,return_sequences=True,recurrent_dropout=0.2))\n",
    "# model.add(LSTM(128,return_sequences=True,recurrent_dropout=0.2))\n",
    "# model.add(LSTM(128))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(VOCAB_SIZE,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bLTN-czYnWKk",
    "outputId": "f07f9e47-3591-4bab-8124-159a5ba1ecdf"
   },
   "outputs": [],
   "source": [
    "if os.path.exists('weights/'):\n",
    "  print(\"Weights folder already exists\")\n",
    "else:\n",
    "  os.mkdir('weights/')\n",
    "  print(\"weights folder created\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['acc'])\n",
    "filepath=\"weights/weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,'acc',1,True,mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z4Bb5XRNJHK"
   },
   "outputs": [],
   "source": [
    "#Generator function to divide into batches and train manually\n",
    "def text_generator_v3(X, labels,vocab_size, batchsize, mode=\"train\"):\n",
    "    while True:\n",
    "        start = 0\n",
    "        end = batchsize\n",
    "\n",
    "        while start  < len(X): \n",
    "            x = X[start:end] \n",
    "            y = labels[start:end]\n",
    "            y = tensorflow.keras.utils.to_categorical(y,vocab_size)\n",
    "            yield x, y\n",
    "\n",
    "            start += batchsize\n",
    "            end += batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7Y8C-NCGZqbV",
    "outputId": "0a26b301-a1c1-4d80-b7ab-b58a90c8a8fe"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.fit_generator(text_generator_v3(X_pad,y,VOCAB_SIZE,BATCH_SIZE),steps_per_epoch=X_pad.shape[0]//BATCH_SIZE,epochs=EPOCHS,verbose=1,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xWZ0hsgnyZY"
   },
   "outputs": [],
   "source": [
    "def generate_text(text,model,tokenizer,reverse_word_index):\n",
    "  intext=text.lower()\n",
    "  # increment=0\n",
    "  sentence=string\n",
    "  sys.stdout.write(text+' ')\n",
    "  while(True):\n",
    "    seq=tokenizer.texts_to_sequences([intext])\n",
    "    pad=pad_sequences(seq,maxlen=10,padding='post',truncating='pre')\n",
    "    pred=model.predict(pad)\n",
    "    # top=model.predict_classes(pad)\n",
    "    top2=pred[0].argsort()[-2:][::-1]\n",
    "    word=np.random.choice(top2,p=[0.7,0.3])\n",
    "    word=reverse_word_index[word]\n",
    "    sys.stdout.write(word+' ')\n",
    "    sentence = ' '.join(word)\n",
    "    intext=intext+' '+word\n",
    "    # increment=increment+1\n",
    "    if(word=='.'):\n",
    "      break\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kecJP7wp5wu_"
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# model.load_weights('weights/weights-improvement-10-0.44.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1eq2PK0c5u_S",
    "outputId": "477ca907-040f-480d-f6ee-f55c52c0dc61"
   },
   "outputs": [],
   "source": [
    "text='Their lights glitter within'\n",
    "gen_text=generate_text(text,model,tokenizer,reverse_word_index)\n",
    "# generated_sentences.append(\"epoch : 2 \"+gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBNzyioh9hOT"
   },
   "source": [
    "#                        Character Based Text Generation for Autocompleting Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uftymr57LDDb"
   },
   "outputs": [],
   "source": [
    "MAXLEN = 25\n",
    "EMBEDDING_DIM = 32\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur5LQpSArvKq"
   },
   "outputs": [],
   "source": [
    "raw_text=load_text('internet_archive_scifi_v3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rusr5cQ5nv3e"
   },
   "outputs": [],
   "source": [
    "def clean_text_char(text):\n",
    "  text=re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "  text=text.replace('.',' .')\n",
    "  tokens = text.split()\n",
    "  punc=string.punctuation\n",
    "  punc=punc.replace('.','')\n",
    "  punc=punc + str(\"\\\\\")\n",
    "  table=str.maketrans('','',punc)\n",
    "  tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "  tokens = (word for word in tokens if not word.isdigit())\n",
    "\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "\n",
    "  text = ' '.join(tokens)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXKTKSsgoAw-"
   },
   "outputs": [],
   "source": [
    "upd_text=clean_text_char(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJKSv4d-8sb7"
   },
   "outputs": [],
   "source": [
    "#Creating dictionary for char vocabulary\n",
    "chars=sorted(list(set(upd_text)))\n",
    "char_to_int = dict((c, i) for i,c in enumerate(chars))\n",
    "int_to_char = dict((c,i) for c,i in enumerate(chars)) \n",
    "vocab_size=len(char_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16bCSXKKz6FZ"
   },
   "outputs": [],
   "source": [
    "def generate_sentences(tokens):\n",
    "  sentences=list()\n",
    "  sentence=list()\n",
    "  for word in tokens:\n",
    "    if(word=='.'):\n",
    "      sentence.append(word)\n",
    "      if(len(sentence)<5):\n",
    "        sentence=list()\n",
    "        continue\n",
    "      sentences.append(sentence)\n",
    "      sentence=list()\n",
    "    else:\n",
    "      sentence.append(word)\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOruMTpEszjv"
   },
   "outputs": [],
   "source": [
    "sentences=generate_sentences(upd_text.split()[700:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yiEpl2SuDLg"
   },
   "outputs": [],
   "source": [
    "def generate_char_sequences(sentences):\n",
    "  sequences=list()\n",
    "  split = 0.25\n",
    "  X,y=list(),list()\n",
    "  for i in range(len(sentences)):\n",
    "    sentences[i]=' '.join(sentences[i])\n",
    "  for sentence in sentences:\n",
    "    # interval=len(sentence)-int(len(sentence)*split)\n",
    "    interval=int(len(sentence)*split)\n",
    "    for i in range(0,len(sentence)-interval):\n",
    "      x_seq=sentence[i:i+interval]\n",
    "      y_seq=sentence[i+interval]\n",
    "      X.append([char_to_int[char] for char in x_seq])\n",
    "      y.append(char_to_int[y_seq])\n",
    "  return X,y\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sS_e1U221S0a"
   },
   "outputs": [],
   "source": [
    "#Generate character sequences \n",
    "X,y=generate_char_sequences(sentences[:500000])\n",
    "X,y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ceETz1upy6kn",
    "outputId": "295bfc68-3623-4c1c-c6ff-146590194e45"
   },
   "outputs": [],
   "source": [
    "data_folder='data/'\n",
    "if os.path.exists(data_folder):\n",
    "  print(\"Folder exists\")\n",
    "else:\n",
    "  os.mkdir(data_folder)\n",
    "  print(\"Data folder created\")\n",
    "\n",
    "filename='X_y_data.npz'\n",
    "print(os.path.join(data_folder,filename))\n",
    "#Saving the generated sequences\n",
    "np.savez_compressed(os.path.join(data_folder,filename),a=X,b=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEFSYuBaiEC0"
   },
   "outputs": [],
   "source": [
    "data=np.load(os.path.join(data_folder,filename),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7BId-mQiTN8"
   },
   "outputs": [],
   "source": [
    "X=data['a']\n",
    "y=data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdH8bZu5_dee"
   },
   "outputs": [],
   "source": [
    "X=pad_sequences(X,maxlen=25,padding='post',truncating='pre')\n",
    "y = y.reshape((-1,1))\n",
    "y = tensorflow.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcZ9fMyZwMQe"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,32,input_length=MAXLEN))\n",
    "model.add(CuDNNLSTM(128,return_sequences=True))\n",
    "model.add(CuDNNLSTM(128,return_sequences=True))\n",
    "model.add(CuDNNLSTM(128,return_sequences=True))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "101x2FeiA2yd"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['acc'])\n",
    "# os.mkdir('drive/My Drive/SciFi Text Generation/weights/char_700k_weights')\n",
    "call_backs=ModelCheckpoint('drive/My Drive/SciFi Text Generation/weights/char_500k_weights/weights-improvement-{epoch:02d}-{acc:.2f}.hdf5',monitor='acc',verbose=1,save_best_only=True,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "8gkBbY10BVKa",
    "outputId": "1fbd7a50-2027-433e-ff45-92a230a8e71f"
   },
   "outputs": [],
   "source": [
    "model.fit(X,y,batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1,callbacks=[call_backs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFcn6auJU8p3"
   },
   "outputs": [],
   "source": [
    "#Function for creating batches\n",
    "# def generator_v4(sentences, batch_size,vocab_size,maxlen=25):\n",
    "#   sentences=np.array(sentences)\n",
    "#   while True:\n",
    "#     idx = np.random.choice(len(sentences),batch_size)\n",
    "#     X,y = generate_char_sequences(sentences[[idx]])\n",
    "#     X,y = np.array(X), np.array(y)\n",
    "#     X = pad_sequences(X, maxlen=maxlen, padding='post', truncating='pre')\n",
    "#     y = tensorflow.keras.utils.to_categorical(y,vocab_size)\n",
    "#     yield X,y\n",
    "\n",
    "\n",
    "# batch_size=512\n",
    "# model.fit_generator(generator_v4(sentences[:500000],batch_size,vocab_size),steps_per_epoch=len(sentences[:500000])//batch_size,epochs=100,verbose=1,callbacks=[call_backs])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APVtU5hWBlga"
   },
   "outputs": [],
   "source": [
    "def generate_text_char(text,model,char_to_int,int_to_char):\n",
    "  intext=list()\n",
    "  sys.stdout.write(text)\n",
    "  text=text.lower()\n",
    "  for char in text:\n",
    "    intext.append(char_to_int[char])\n",
    "  # intext=np.array(intext)\n",
    "  while(True):\n",
    "    pad=pad_sequences([intext],maxlen=25,padding='pre',truncating='pre')\n",
    "    pred=model.predict(pad)\n",
    "    top2=pred[0].argsort()[-2:][::-1]\n",
    "    choice=np.random.choice(top2,p=[0.7,0.3])\n",
    "    char=int_to_char[choice]\n",
    "    sys.stdout.write(char)\n",
    "    intext.append(choice)\n",
    "    if(char=='.'):\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LLgLrMGxy-sO",
    "outputId": "1ac792b6-08c5-4651-e255-64eb71044a6f"
   },
   "outputs": [],
   "source": [
    "text='Beneath the willow there are'\n",
    "generate_text_char(text,model,char_to_int,int_to_char)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SciFi Auto Completer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
